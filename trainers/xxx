
|- models
	|- bert_modules
		|- attention
			|- __init__.py
				from .bert import BERTTrainer
                TRAINERS = {
                    BERTTrainer.code(): BERTTrainer,
                }

                def trainer_factory(args, model, train_loader, val_loader, test_loader, export_root):
                    trainer = TRAINERS[args.trainer_code]
                    return trainer(args, model, train_loader, val_loader, test_loader, export_root)
			|- multi_head.py
                import torch.nn as nn
                from .single import Attention


                class MultiHeadedAttention(nn.Module):
                    """
                    Take in model size and number of heads.
                    """

                    def __init__(self, h, d_model, dropout=0.1):
                        super().__init__()
                        assert d_model % h == 0

                        # We assume d_v always equals d_k
                        self.d_k = d_model // h
                        self.h = h

                        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])
                        self.output_linear = nn.Linear(d_model, d_model)
                        self.attention = Attention()

                        self.dropout = nn.Dropout(p=dropout)

                    def forward(self, query, key, value, mask=None):
                        batch_size = query.size(0)

                        # 1) Do all the linear projections in batch from d_model => h x d_k
                        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
                                            for l, x in zip(self.linear_layers, (query, key, value))]

                        # 2) Apply attention on all the projected vectors in batch.
                        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)

                        # 3) "Concat" using a view and apply a final linear.
                        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)

                        return self.output_linear(x), attn

			|- single.py
                import torch.nn as nn
                import torch.nn.functional as F
                import torch

                import math

                class Attention(nn.Module):
                    """
                    Compute 'Scaled Dot Product Attention
                    """

                    def forward(self, query, key, value, mask=None, dropout=None):
                        scores = torch.matmul(query, key.transpose(-2, -1)) \
                                / math.sqrt(query.size(-1))

                        if mask is not None:
                            scores = scores.masked_fill(mask == 0, -1e9)

                        p_attn = F.softmax(scores, dim=-1)

                        if dropout is not None:
                            p_attn = dropout(p_attn)

                        return torch.matmul(p_attn, value), p_attn
                    
		|- embedding
			|- __init__.py
                from .bert import BERTEmbedding
			|- bert.py
                import torch.nn as nn
                from .token import TokenEmbedding
                from .position import PositionalEmbedding


                class BERTEmbedding(nn.Module):
                    """
                    BERT Embedding which is consisted with under features
                        1. TokenEmbedding : normal embedding matrix
                        2. PositionalEmbedding : adding positional information using sin, cos
                        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)

                        sum of all these features are output of BERTEmbedding
                    """

                    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):
                        """
                        :param vocab_size: total vocab size
                        :param embed_size: embedding size of token embedding
                        :param dropout: dropout rate
                        """
                        super().__init__()
                        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)
                        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)
                        # self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)
                        self.dropout = nn.Dropout(p=dropout)
                        self.embed_size = embed_size

                    def forward(self, sequence):
                        x = self.token(sequence) + self.position(sequence)  # + self.segment(segment_label)
                        return self.dropout(x)

			|- position.py
                import torch.nn as nn
                import torch
                import math


                class PositionalEmbedding(nn.Module):

                    def __init__(self, max_len, d_model):
                        super().__init__()

                        # Compute the positional encodings once in log space.
                        self.pe = nn.Embedding(max_len, d_model)

                    def forward(self, x):
                        batch_size = x.size(0)
                        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)

			|- token.py
                import torch.nn as nn


                class TokenEmbedding(nn.Embedding):
                    def __init__(self, vocab_size, embed_size=512):
                        super().__init__(vocab_size, embed_size, padding_idx=0)

		|- utils
			|- __init__.py
                from .feed_forward import PositionwiseFeedForward
                from .layer_norm import LayerNorm
                from .sublayer import SublayerConnection
                from .gelu import GELU

			|- feed_forward.py
                import torch.nn as nn
                from .gelu import GELU


                class PositionwiseFeedForward(nn.Module):
                    "Implements FFN equation."

                    def __init__(self, d_model, d_ff, dropout=0.1):
                        super(PositionwiseFeedForward, self).__init__()
                        self.w_1 = nn.Linear(d_model, d_ff)
                        self.w_2 = nn.Linear(d_ff, d_model)
                        self.dropout = nn.Dropout(dropout)
                        self.activation = GELU()

                    def forward(self, x):
                        return self.w_2(self.dropout(self.activation(self.w_1(x))))

			|- gelu.py
                import torch.nn as nn
                import torch
                import math


                class GELU(nn.Module):

                    def forward(self, x):
                        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))

			|- layer_norm.py
                import torch.nn as nn
                import torch


                class LayerNorm(nn.Module):
                    "Construct a layernorm module (See citation for details)."

                    def __init__(self, features, eps=1e-6):
                        super(LayerNorm, self).__init__()
                        self.a_2 = nn.Parameter(torch.ones(features))
                        self.b_2 = nn.Parameter(torch.zeros(features))
                        self.eps = eps

                    def forward(self, x):
                        mean = x.mean(-1, keepdim=True)
                        std = x.std(-1, keepdim=True)
                        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

			|- sublayer.py
                import torch.nn as nn
                from .layer_norm import LayerNorm


                class SublayerConnection(nn.Module):
                    """
                    A residual connection followed by a layer norm.
                    Note for code simplicity the norm is first as opposed to last.
                    """

                    def __init__(self, size, dropout):
                        super(SublayerConnection, self).__init__()
                        self.norm = LayerNorm(size)
                        self.dropout = nn.Dropout(dropout)

                    def forward(self, x, sublayer):
                        "Apply residual connection to any sublayer with the same size."
                        return x + self.dropout(sublayer(self.norm(x)))

		|- __init__.py

		|- bert.py
            from torch import nn as nn

            from models.bert_modules.embedding import BERTEmbedding
            from models.bert_modules.transformer import TransformerBlock
            from utils import fix_random_seed_as


            class BERT(nn.Module):
                def __init__(self, args):
                    super().__init__()

                    fix_random_seed_as(args.model_init_seed)
                    # self.init_weights()

                    max_len = args.bert_max_len
                    num_items = args.num_items
                    n_layers = args.bert_num_blocks
                    heads = args.bert_num_heads
                    vocab_size = num_items + 2
                    hidden = args.bert_hidden_units
                    self.hidden = hidden
                    dropout = args.bert_dropout

                    # embedding for BERT, sum of positional, segment, token embeddings
                    self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden, max_len=max_len, dropout=dropout)

                    # multi-layers transformer blocks, deep network
                    self.transformer_blocks = nn.ModuleList(
                        [TransformerBlock(hidden, heads, hidden * 4, dropout) for _ in range(n_layers)])

                def forward(self, x):
                    mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)
                    x = self.embedding(x)

                    all_attn_weights = []
                    for transformer in self.transformer_blocks:
                        x, attn_weights = transformer.forward(x, mask)
                        all_attn_weights.append(attn_weights)

                    return x, all_attn_weights


                def init_weights(self):
                    pass


		|- transformer.py
            import torch.nn as nn

            from .attention import MultiHeadedAttention
            from .utils import SublayerConnection, PositionwiseFeedForward


            class TransformerBlock(nn.Module):
                """
                Bidirectional Encoder = Transformer (self-attention)
                Transformer = MultiHead_Attention + Feed_Forward with sublayer connection
                """

                def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):
                    """
                    :param hidden: hidden size of transformer
                    :param attn_heads: head sizes of multi-head attention
                    :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size
                    :param dropout: dropout rate
                    """

                    super().__init__()
                    self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)
                    self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)
                    self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)
                    self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)
                    self.dropout = nn.Dropout(p=dropout)

                def forward(self, x, mask):
                    x, attn_weights = self.attention.forward(x, x, x, mask=mask)  # Capture attention weights here
                    x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask)[0])  # Ignore weights
                    x = self.output_sublayer(x, self.feed_forward)
                    return self.dropout(x), attn_weights



	|- __init__.py
        from .bert import BERTModel

        MODELS = {
            BERTModel.code(): BERTModel
        }


        def model_factory(args):
            model = MODELS[args.model_code]
            return model(args)

	|- base.py
        import torch.nn as nn

        from abc import *


        class BaseModel(nn.Module, metaclass=ABCMeta):
            def __init__(self, args):
                super().__init__()
                self.args = args

            @classmethod
            @abstractmethod
            def code(cls):
                pass

	|- bert.py
        from .base import BaseModel
        from .bert_modules.bert import BERT

        import torch.nn as nn


        class BERTModel(BaseModel):
            def __init__(self, args):
                super().__init__(args)
                self.bert = BERT(args)
                self.out = nn.Linear(self.bert.hidden, args.num_items + 1)

            @classmethod
            def code(cls):
                return 'bert'

            def forward(self, x):
                x = self.bert(x)
                return self.out(x)

|- trainers
	|- __init__.py
        from .bert import BERTTrainer


        TRAINERS = {
            BERTTrainer.code(): BERTTrainer,
        }


        def trainer_factory(args, model, train_loader, val_loader, test_loader, export_root):
            trainer = TRAINERS[args.trainer_code]
            return trainer(args, model, train_loader, val_loader, test_loader, export_root)

	|- base.py
        from pprint import pprint
        from loggers import *
        from config import STATE_DICT_KEY, OPTIMIZER_STATE_DICT_KEY
        from utils import AverageMeterSet

        import torch
        import torch.nn as nn
        import torch.optim as optim
        import torch.nn.functional as F
        from torch.utils.tensorboard import SummaryWriter
        from tqdm import tqdm

        import json
        from abc import *
        from pathlib import Path


        class AbstractTrainer(metaclass=ABCMeta):
            def __init__(self, args, model, train_loader, val_loader, test_loader, export_root):
                self.args = args
                self.device = args.device
                self.model = model.to(self.device)
                self.is_parallel = args.num_gpu > 1
                if self.is_parallel:
                    self.model = nn.DataParallel(self.model)

                self.train_loader = train_loader
                self.val_loader = val_loader
                self.test_loader = test_loader
                self.optimizer = self._create_optimizer()
                if args.enable_lr_schedule:
                    self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.decay_step, gamma=args.gamma)

                self.num_epochs = args.num_epochs
                self.metric_ks = args.metric_ks
                self.best_metric = args.best_metric

                self.export_root = export_root
                self.writer, self.train_loggers, self.val_loggers = self._create_loggers()
                self.add_extra_loggers()
                self.logger_service = LoggerService(self.train_loggers, self.val_loggers)
                self.log_period_as_iter = args.log_period_as_iter

            @abstractmethod
            def add_extra_loggers(self):
                pass

            @abstractmethod
            def log_extra_train_info(self, log_data):
                pass

            @abstractmethod
            def log_extra_val_info(self, log_data):
                pass

            @classmethod
            @abstractmethod
            def code(cls):
                pass

            @abstractmethod
            def calculate_loss(self, batch):
                pass

            @abstractmethod
            def calculate_metrics(self, batch):
                pass

            def train(self):
                accum_iter = 0
                self.validate(0, accum_iter)
                for epoch in range(self.num_epochs):
                    accum_iter = self.train_one_epoch(epoch, accum_iter)
                    self.validate(epoch, accum_iter)
                self.logger_service.complete({
                    'state_dict': (self._create_state_dict()),
                })
                self.writer.close()

            def train_one_epoch(self, epoch, accum_iter):
                self.model.train()


                average_meter_set = AverageMeterSet()
                tqdm_dataloader = tqdm(self.train_loader)

                for batch_idx, batch in enumerate(tqdm_dataloader):
                    batch_size = batch[0].size(0)
                    batch = [x.to(self.device) for x in batch]

                    self.optimizer.zero_grad()
                    loss = self.calculate_loss(batch)
                    loss.backward()

                    self.optimizer.step()
                    if self.args.enable_lr_schedule:
                        self.lr_scheduler.step()

                    average_meter_set.update('loss', loss.item())
                    tqdm_dataloader.set_description(
                        'Epoch {}, loss {:.3f} '.format(epoch+1, average_meter_set['loss'].avg))

                    accum_iter += batch_size

                    if self._needs_to_log(accum_iter):
                        tqdm_dataloader.set_description('Logging to Tensorboard')
                        log_data = {
                            'state_dict': (self._create_state_dict()),
                            'epoch': epoch+1,
                            'accum_iter': accum_iter,
                        }
                        log_data.update(average_meter_set.averages())
                        self.log_extra_train_info(log_data)
                        self.logger_service.log_train(log_data)
                    
                    if batch_idx == 1 and epoch == 0:
                        with torch.no_grad():
                            seqs, labels = batch
                            logits = self.model(seqs)  # B x T x V
                            softmax_probs = F.softmax(logits, dim=-1)  # Apply softmax to get probabilities
                            predictions = torch.argmax(softmax_probs, dim=-1)  # Get the predicted items

                            print("\n\033[92mExample Model Output (logits):\033[0m")
                            print(logits[0])  # Print logits for the first sequence in the batch
                            print("\033[92mExample Model Output (softmax probabilities):\033[0m")
                            print(softmax_probs[0])  # Print softmax probabilities for the first sequence
                            print("\033[92mExample Model Predictions:\033[0m")
                            print(predictions[0])  # Print predictions for the first sequence
                            print("\033[91m" + "-" * 50 + "\033[0m")

                # Save attention weights after finishing the epoch
                if epoch == self.num_epochs - 1:
                    self.save_attention_weights()

                return accum_iter

            def save_attention_weights(self):
                self.model.eval()
                all_attention_weights = []
                with torch.no_grad():
                    for batch in self.test_loader:
                        batch = [x.to(self.device) for x in batch]
                        outputs = self.model(batch[0])
                        # If there's no attention_weights, use `outputs` directly or handle accordingly
                        all_attention_weights.extend(outputs)  # Adjust as needed

                # Save the attention weights to files
                for idx, weights in enumerate(all_attention_weights):
                    torch.save(weights.cpu(), f"attention_weights_epoch_{self.num_epochs}_user_{idx}.pt")

            def validate(self, epoch, accum_iter):
                self.model.eval()

                average_meter_set = AverageMeterSet()

                with torch.no_grad():
                    tqdm_dataloader = tqdm(self.val_loader)
                    for batch_idx, batch in enumerate(tqdm_dataloader):
                        batch = [x.to(self.device) for x in batch]

                        metrics = self.calculate_metrics(batch)

                        for k, v in metrics.items():
                            average_meter_set.update(k, v)
                        description_metrics = ['NDCG@%d' % k for k in self.metric_ks[:3]] +\
                                            ['Recall@%d' % k for k in self.metric_ks[:3]] +\
                                            ['HR@%d' % k for k in self.metric_ks[:3]]
                        description = 'Val: ' + ', '.join(s + ' {:.3f}' for s in description_metrics)
                        description = description.replace('NDCG', 'N').replace('Recall', 'R').replace('HR', 'H')
                        description = description.format(*(average_meter_set[k].avg for k in description_metrics))
                        tqdm_dataloader.set_description(description)

                    log_data = {
                        'state_dict': (self._create_state_dict()),
                        'epoch': epoch+1,
                        'accum_iter': accum_iter,
                    }
                    log_data.update(average_meter_set.averages())
                    self.log_extra_val_info(log_data)
                    self.logger_service.log_val(log_data)

            def test(self):
                print('Test best model with test set!')

                best_model = torch.load(os.path.join(self.export_root, 'models', 'best_acc_model.pth')).get('model_state_dict')
                self.model.load_state_dict(best_model)
                self.model.eval()

                average_meter_set = AverageMeterSet()

                with torch.no_grad():
                    tqdm_dataloader = tqdm(self.test_loader)
                    for batch_idx, batch in enumerate(tqdm_dataloader):
                        batch = [x.to(self.device) for x in batch]

                        metrics = self.calculate_metrics(batch)

                        for k, v in metrics.items():
                            average_meter_set.update(k, v)
                        description_metrics = ['NDCG@%d' % k for k in self.metric_ks[:3]] +\
                                            ['Recall@%d' % k for k in self.metric_ks[:3]] +\
                                            ['HR@%d' % k for k in self.metric_ks[:3]]
                        description = 'Test: ' + ', '.join(s + ' {:.3f}' for s in description_metrics)
                        description = description.replace('NDCG', 'N').replace('Recall', 'R').replace('HR', 'H')
                        description = description.format(*(average_meter_set[k].avg for k in description_metrics))
                        tqdm_dataloader.set_description(description)

                    average_metrics = average_meter_set.averages()
                    with open(os.path.join(self.export_root, 'logs', 'test_metrics.json'), 'w') as f:
                        json.dump(average_metrics, f, indent=4)
                    pprint(average_metrics)

            def _create_optimizer(self):
                args = self.args
                if args.optimizer.lower() == 'adam':
                    return optim.Adam(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
                elif args.optimizer.lower() == 'sgd':
                    return optim.SGD(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum)
                else:
                    raise ValueError

            def _create_loggers(self):
                root = Path(self.export_root)
                writer = SummaryWriter(root.joinpath('logs'))
                model_checkpoint = root.joinpath('models')

                train_loggers = [
                    MetricGraphPrinter(writer, key='epoch', graph_name='Epoch', group_name='Train'),
                    MetricGraphPrinter(writer, key='loss', graph_name='Loss', group_name='Train'),
                ]

                val_loggers = []
                for k in self.metric_ks:
                    val_loggers.append(
                        MetricGraphPrinter(writer, key='NDCG@%d' % k, graph_name='NDCG@%d' % k, group_name='Validation'))
                    val_loggers.append(
                        MetricGraphPrinter(writer, key='Recall@%d' % k, graph_name='Recall@%d' % k, group_name='Validation'))
                val_loggers.append(RecentModelLogger(model_checkpoint))
                val_loggers.append(BestModelLogger(model_checkpoint, metric_key=self.best_metric))
                return writer, train_loggers, val_loggers

            def _create_state_dict(self):
                return {
                    STATE_DICT_KEY: self.model.module.state_dict() if self.is_parallel else self.model.state_dict(),
                    OPTIMIZER_STATE_DICT_KEY: self.optimizer.state_dict(),
                }

            def _needs_to_log(self, accum_iter):
                return accum_iter % self.log_period_as_iter < self.args.train_batch_size and accum_iter != 0

	|- bert.py
        from .base import AbstractTrainer
        from .utils import recalls_and_ndcgs_for_ks

        import torch.nn as nn


        class BERTTrainer(AbstractTrainer):
            def __init__(self, args, model, train_loader, val_loader, test_loader, export_root):
                super().__init__(args, model, train_loader, val_loader, test_loader, export_root)
                self.ce = nn.CrossEntropyLoss(ignore_index=0)

            @classmethod
            def code(cls):
                return 'bert'

            def add_extra_loggers(self):
                pass

            def log_extra_train_info(self, log_data):
                pass

            def log_extra_val_info(self, log_data):
                pass

            def calculate_loss(self, batch):
                seqs, labels = batch
                logits = self.model(seqs)  # B x T x V

                logits = logits.view(-1, logits.size(-1))  # (B*T) x V
                labels = labels.view(-1)  # B*T
                loss = self.ce(logits, labels)
                return loss

            def calculate_metrics(self, batch):
                seqs, candidates, labels = batch
                scores = self.model(seqs)  # B x T x V
                scores = scores[:, -1, :]  # B x V
                scores = scores.gather(1, candidates)  # B x C

                metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)
                return metrics

	|- utils.py
        import torch


        def recall(scores, labels, k):
            scores = scores
            labels = labels
            rank = (-scores).argsort(dim=1)
            cut = rank[:, :k]
            hit = labels.gather(1, cut)
            return (hit.sum(1).float() / torch.min(torch.Tensor([k]).to(hit.device), labels.sum(1).float())).mean().cpu().item()


        def ndcg(scores, labels, k):
            scores = scores.cpu()
            labels = labels.cpu()
            rank = (-scores).argsort(dim=1)
            cut = rank[:, :k]
            hits = labels.gather(1, cut)
            position = torch.arange(2, 2+k)
            weights = 1 / torch.log2(position.float())
            dcg = (hits.float() * weights).sum(1)
            idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in labels.sum(1)])
            ndcg = dcg / idcg
            return ndcg.mean()

        def hit_rate(scores, labels, k):
            scores = scores
            labels = labels
            rank = (-scores).argsort(dim=1)
            cut = rank[:, :k]
            hits = labels.gather(1, cut)
            return (hits.sum(1) > 0).float().mean().cpu().item()

        def recalls_and_ndcgs_for_ks(scores, labels, ks):
            metrics = {}
            scores = scores
            labels = labels
            answer_count = labels.sum(1)
            labels_float = labels.float()
            rank = (-scores).argsort(dim=1)
            cut = rank
            for k in sorted(ks, reverse=True):
                cut = cut[:, :k]
                hits = labels_float.gather(1, cut)
                metrics['Recall@%d' % k] = \
                    (hits.sum(1) / torch.min(torch.Tensor([k]).to(labels.device), labels.sum(1).float())).mean().cpu().item()
                position = torch.arange(2, 2+k)
                weights = 1 / torch.log2(position.float())
                dcg = (hits * weights.to(hits.device)).sum(1)
                idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in answer_count]).to(dcg.device)
                ndcg = (dcg / idcg).mean()
                metrics['NDCG@%d' % k] = ndcg.cpu().item()
                metrics['HR@%d' % k] = hit_rate(scores, labels, k)
            return metrics
|- main.py
    import torch
    import distutils.version
    from options import args
    from models import model_factory
    from dataloaders import dataloader_factory
    from trainers import trainer_factory
    from utils import *


    if __name__ == '__main__':
        print("≽^•༚• ྀི≼ Use ml - 1m dataset ـــــــــــــــﮩ٨ـ❤️ﮩ٨ـﮩﮩ٨ـ")
        export_root = setup_train(args)
        train_loader, val_loader, test_loader = dataloader_factory(args)
        model = model_factory(args)
        trainer = trainer_factory(args, model, train_loader, val_loader, test_loader, export_root)
        trainer.train()
        trainer.test()